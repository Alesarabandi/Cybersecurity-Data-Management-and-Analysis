{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Title: Data Storage, Integration and Enrichment - With PostgreSQL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 🛠️ Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic/Standard libraries to manage data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To use PostgreSQL in python environment\n",
    "# pip install psycopg2 # (only if you haven't installed this library in your pc)\n",
    "# pip install sqlalchemy # (only if you haven't installed this library in your pc)\n",
    "import psycopg2 # A PostgreSQL adapter for Python, for interacting with PostgreSQL databases\n",
    "from psycopg2 import sql # Importing the sql module from psycopg2 to help construct SQL queries safely and dynamically\n",
    "from sqlalchemy import create_engine # Importing the create_engine function from SQLAlchemy, which helps establish database connections using various database engines\n",
    "\n",
    "# pip install tqdm # (only if you haven't installed this library in your pc)\n",
    "from tqdm import tqdm # To add a progress bar when you use a for cycle (useful when you want to take trace of the iterations in a cycle or its execution time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 🗃️ Data Storage & Integration\n",
    "In our case, the tables Hackers, Targets, and Attacks have been designed to represent attackers, victims, and the relationships between them, respectively. With the rigid structure of relational databases, the use of primary and foreign keys, and PostgreSQL as the database management system, integrity, scalability, and ease of data analysis are ensured. This approach is particularly advantageous for scenarios where the data schema is fixed and involves processing large datasets.\n",
    "\n",
    "#### **Advantages of storing data in a relational database**\n",
    "- ##### **Why a relational database is ideal?**\n",
    "   1. **Structure and integrity:**\n",
    "      - Fixed schema ensures consistency.\n",
    "      - Primary and foreign keys enforce referential integrity, preventing duplicates or invalid data.\n",
    "\n",
    "   2. **Query efficiency:**\n",
    "      - SQL enables complex queries for aggregation and filtering, essential for EDA.\n",
    "\n",
    "   3. **Processing efficiency:**\n",
    "      - Relational databases handle large datasets efficiently.\n",
    "\n",
    "   4. **Scalability and management:**\n",
    "      - Easily manages 40,000 observations per table without performance issues.\n",
    "\n",
    "- ##### **Why PostgreSQL is an ideal choice?**\n",
    "   1. **Performance and scalability:**\n",
    "      - Optimized for large datasets.\n",
    "\n",
    "   2. **Support for complex operations:**\n",
    "      - Offers advanced SQL functionalities for analytical tasks.\n",
    "\n",
    "   3. **Concurrency management:**\n",
    "      - MVCC enables simultaneous data access.\n",
    "\n",
    "   4. **Open-source and extensibility:**\n",
    "      - Free and customizable with extensions.\n",
    "      \n",
    "   5. **Relationship management:**\n",
    "      - Full support for foreign keys, making it suitable for complex relational models.\n",
    "      \n",
    "#### **Database Engineering**\n",
    "- ##### **Database Design Considerations**\n",
    "   To ensure clarity regarding the design of our database, we emphasize that it has been tailored specifically to the nature of the data we collected. Here are the key considerations that guided our design decisions:\n",
    "\n",
    "   **Static Nature of the Data**\n",
    "   - The data we are working with is **static** and will not be updated in the future.\n",
    "   - The primary goal of this project is to answer the research question defined at the beginning, relying on **Exploratory Data Analysis (EDA)** of the current dataset.\n",
    "\n",
    "   **Relationships Between Entities**\n",
    "   Based on the structure and characteristics of our data:\n",
    "   - **One hacker** is related to **one and only one attack**, which in turn is related to **one and only one target**.\n",
    "   - Conversely, **one target** is related to **one and only one attack**, which is subsequently related to **one and only one hacker**.\n",
    "\n",
    "   **Implications on Database Design**\n",
    "   - This strict **one-to-one relationship** between hackers, attacks, and targets simplifies the design of our database, as each entity corresponds uniquely to another.\n",
    "   - **Foreign key relationships** remain critical to enforce referential integrity and maintain the linkage between the entities.\n",
    "\n",
    "   This clear understanding of the data and its relationships ensures that our database is both optimized for the specific analysis at hand and appropriately structured to reflect the unique nature of the dataset.\n",
    "\n",
    "- ##### **Relation between entities**\n",
    "   - **Entity `Hackers`:** Represents attackers initiating an attack. Here there are information about hackers. This info are collected (or more pecisely generated) tanks to the use of an API (for more info see documentation/report)\n",
    "      - **Primary key:** `att_ip`\n",
    "\n",
    "   - **Entity `Targets`:** Represents the attack victims. Here there are information about victims. Also this info are collected (or more pecisely generated) tanks to the use of the same API mentioned before.\n",
    "      - **Primary key:** `vic_ip`\n",
    "  \n",
    "   - **Relation `Attacks`:** This table connects `Hackers` to `Targets` by representing specific attacks. It includes:\n",
    "      - **Foreign key 1:** `Source IP Address` (linked to `Hackers.att_ip`)\n",
    "      - **Foreign key 2:** `Destination IP Address` (linked to `Targets.vic_ip`)\n",
    "\n",
    "- ##### **Entity-Relationship (ER) Model**\n",
    "   - **Entities:**\n",
    "      - `Hackers(att_ip, ... other attributes)`\n",
    "      - `Targets(vic_ip, ... other attributes)`\n",
    "\n",
    "   - **Relation:**\n",
    "      - `Attacks(Source IP Address, Destination IP Address, ... other attributes)`\n",
    "\n",
    "   - **Cardinality:**\n",
    "      - Each **Hacker** is related to **one and only one Attack** (1:1).\n",
    "      - Each **Attack** is related to **one and only one Target** (1:1).  \n",
    "\n",
    "- ##### **Logical model**\n",
    "   - **Tables:**\n",
    "      1. `Hackers(att_ip [PK], att_continent_name, att_country_name, ...)`\n",
    "      2. `Targets(vic_ip [PK], vic_continent_name, vic_country_name, ...)`\n",
    "      3. `Attacks(Source IP Address [FK -> Hackers.att_ip], Destination IP Address [FK -> Targets.vic_ip], Protocol, Severity Level, Attack Type, ...)`\n",
    "      \n",
    "   - **Relationships:**\n",
    "      - `Hackers.att_ip` → `Attacks.Source IP Address`\n",
    "      - `Targets.vic_ip` → `Attacks.Destination IP Address`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database credentials\n",
    "db_config = {\n",
    "    \"dbname\": \"DataMan_DB\",       # Name of your database\n",
    "    \"user\": \"postgres\",           # PostgreSQL user\n",
    "    \"password\": \"dataAcc0\",       # User password\n",
    "    \"host\": \"localhost\",          # Database host (local)\n",
    "    \"port\": \"5432\"                # Default port for PostgreSQL\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to import CSV files into PostgreSQL database\n",
    "The function `import_CSV_as_TABLE` imports a CSV file into a PostgreSQL database as a table using the `SQLAlchemy` library for database interaction and `Pandas` for reading and handling the CSV data. \n",
    "\n",
    "##### **Key Features**:\n",
    "1. **Database Connection**:\n",
    "   - Establishes a connection to the PostgreSQL database using a SQLAlchemy engine created from the `db_config` dictionary, which contains database credentials (`user`, `password`, `host`, `port`, and `dbname`).\n",
    "\n",
    "2. **CSV to DataFrame**:\n",
    "   - Reads the specified CSV file (`csv_file_path`) into a Pandas DataFrame for easy manipulation.\n",
    "\n",
    "3. **Table Creation**:\n",
    "   - Writes the DataFrame into the database as a new table (`table_name`), replacing any existing table with the same name (`if_exists='replace'`).\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Catches and reports any errors that occur during the table creation or data import process.\n",
    "\n",
    "5. **Resource Management**:\n",
    "   - Closes the database engine after the operation is completed to free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_CSV_as_TABLE(csv_file_path, table_name, db_config):\n",
    "\n",
    "    # SQLAlchemy engine creation\n",
    "    engine = create_engine(\n",
    "        f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\"\n",
    "    )\n",
    "\n",
    "    # From CSV to a Pandas dataframe\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Write the dataframe on the PostgreSQL database (DataMan_DB) as table\n",
    "    try:\n",
    "        df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "        print(f\"Table '{table_name}' created and populated successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error while importing:\", e)\n",
    "    \n",
    "    # Close the engine\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building process of our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Import files:  33%|███▎      | 1/3 [00:03<00:06,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'Attacks' created and populated successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Import files:  67%|██████▋   | 2/3 [00:06<00:03,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'Hackers' created and populated successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Import files: 100%|██████████| 3/3 [00:08<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'Targets' created and populated successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CSV files path\n",
    "#csv_files_path = r\"D:\\UniVault\\UniProjects\\DataScience\\Year_1\\DataMan\\data\\csv_file\"\n",
    "\n",
    "# A dictionarie that contain as keys the names of the files.csv and as values the names of the new tables that will build in the database\n",
    "#diz_fileNames_tableNames = {\"updated_cybersecurity_attacks.csv\":\"Attacks\",\n",
    "#                            \"attakers_csv_file.csv\":\"Hackers\",\n",
    "#                            \"victims_csv_file.csv\":\"Targets\"\n",
    "#                            }\n",
    "\n",
    "# Importing files into the database\n",
    "#for fileName in tqdm(diz_fileNames_tableNames, desc = \"Import files\"):\n",
    "#    tableName = diz_fileNames_tableNames[fileName]\n",
    "#    csv_file_path = f\"{csv_files_path}\\\\{fileName}\"\n",
    "#    import_CSV_as_TABLE(csv_file_path, tableName, db_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 🧩 Data Enrichment and Quering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to connect and send queries\n",
    "The `execute_query` function is a utility for executing SQL queries on a PostgreSQL database using the `psycopg2` library. \n",
    "\n",
    "##### **Key Features**:\n",
    "\n",
    "1. **Database Connection**:\n",
    "   - Establishes a connection to the PostgreSQL database using `db_config`, which contain the database credentials.\n",
    "\n",
    "2. **Query Execution**:\n",
    "   - Executes the provided SQL query (`query`) using a cursor object.\n",
    "\n",
    "3. **Result Handling**:\n",
    "   - If the query is a `SELECT` statement, it retrieves and prints all results row by row.\n",
    "   - For other query types (e.g., `INSERT`, `UPDATE`, `DELETE`), it commits the changes to the database.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Catches and displays any exceptions that occur during the query execution process.\n",
    "\n",
    "5. **Resource Management**:\n",
    "   - Utilizes Python's `with` statement to ensure that the database connection and cursor are properly closed after execution, even in case of errors.\n",
    "\n",
    "##### **Use Case**:\n",
    "This function is ideal if you just need to see the output of a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query):\n",
    "    try:\n",
    "        # Conneciton to the database\n",
    "        with psycopg2.connect(**db_config) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)  # Executes the query\n",
    "                # If it is a SELECT, retrieve the results\n",
    "                if query.strip().lower().startswith(\"select\"):\n",
    "                    results = cur.fetchall()\n",
    "                    for row in results:\n",
    "                        print(row)\n",
    "                else:\n",
    "                    conn.commit()  # Save changes for queries like INSERT or UPDATE\n",
    "                    print(\"Query successful!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error executing query:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Attacks',)\n",
      "('Hackers',)\n",
      "('Targets',)\n"
     ]
    }
   ],
   "source": [
    "# See all of the tables in your database\n",
    "if __name__ == \"__main__\":\n",
    "    # Query\n",
    "    ExtractTables = \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';\"\n",
    "    execute_query(ExtractTables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '2023-05-30 06:33:58', '103.216.15.12', '84.9.164.252', 31225, 17616, 'ICMP', 503, 'Data', 'HTTP', 'Qui natus odio asperiores nam. Optio nobis iusto accusamus ad perferendis esse at. Asperiores neque at ad.\\nMaiores possimus ipsum saepe vitae. Ad possimus veritatis.', 'IoC Detected', 28.67, 'no', 'Malware', 'Known Pattern B', 'Logged', 'Low', 'Reyansh Dugal', 'Segment A', 'Jamshedpur, Sikkim', '150.9.97.135', 'Log Data', 'No Data', 'Server', 'Mozilla', 'Windows', 2023, 5, 30, 6, 33, 58, 1)\n",
      "(1, '2020-08-26 07:08:30', '78.199.217.198', '66.191.137.154', 17245, 48166, 'ICMP', 1174, 'Data', 'HTTP', 'Aperiam quos modi officiis veritatis rem. Omnis nulla dolore perspiciatis.\\nIllo animi mollitia vero voluptates error ad. Quidem maxime eaque optio a. Consectetur quasi veniam et totam culpa ullam.', 'IoC Detected', 51.5, 'no', 'Malware', 'Known Pattern A', 'Blocked', 'Low', 'Sumer Rana', 'Segment B', 'Bilaspur, Nagaland', 'No proxy', 'Log Data', 'No Data', 'Firewall', 'Mozilla', 'Windows', 2020, 8, 26, 7, 8, 30, 2)\n",
      "(2, '2022-11-13 08:23:25', '63.79.210.48', '198.219.82.17', 16811, 53600, 'UDP', 306, 'Control', 'HTTP', 'Perferendis sapiente vitae soluta. Hic delectus quae nemo ea esse est rerum.', 'IoC Detected', 87.42, 'yes', 'DDoS', 'Known Pattern B', 'Ignored', 'Low', 'Himmat Karpe', 'Segment C', 'Bokaro, Rajasthan', '114.133.48.179', 'Log Data', 'Alert Data', 'Firewall', 'Mozilla', 'Windows', 2022, 11, 13, 8, 23, 25, 6)\n",
      "(3, '2023-07-02 10:38:46', '163.42.196.10', '101.228.192.255', 20018, 32534, 'UDP', 385, 'Data', 'HTTP', 'Totam maxime beatae expedita explicabo porro labore. Minima ab fugit officiis dicta perspiciatis pariatur. Facilis voluptates eligendi dolores eveniet deserunt. Eveniet reprehenderit culpa quo.', 'No Detection', 15.79, 'yes', 'Malware', 'Known Pattern B', 'Blocked', 'Medium', 'Fateh Kibe', 'Segment B', 'Jaunpur, Rajasthan', 'No proxy', 'No Data', 'Alert Data', 'Firewall', 'Mozilla', 'Macintosh', 2023, 7, 2, 10, 38, 46, 6)\n",
      "(4, '2023-07-16 13:11:07', '71.166.185.76', '189.243.174.238', 6131, 26646, 'TCP', 1462, 'Data', 'DNS', 'Odit nesciunt dolorem nisi iste iusto. Animi voluptates soluta quis doloribus quas. Iure harum nihil hic illo repellendus.\\nQuia illo fugit eligendi doloremque. In doloremque autem iure.', 'No Detection', 0.52, 'yes', 'DDoS', 'Known Pattern B', 'Blocked', 'Low', 'Dhanush Chad', 'Segment C', 'Anantapur, Tripura', '149.6.110.119', 'No Data', 'Alert Data', 'Firewall', 'Mozilla', 'Windows', 2023, 7, 16, 13, 11, 7, 6)\n",
      "(5, '2022-10-28 13:14:27', '198.102.5.160', '147.190.155.133', 17430, 52805, 'UDP', 1423, 'Data', 'HTTP', 'Repellat quas illum harum fugit incidunt exercitationem illum. Voluptate asperiores aperiam magnam eius. Eos quis repellat eos.', 'No Detection', 5.76, 'no', 'Malware', 'Known Pattern A', 'Logged', 'Medium', 'Zeeshan Viswanathan', 'Segment C', 'Aurangabad, Meghalaya', 'No proxy', 'No Data', 'No Data', 'Server', 'Opera', 'Linux', 2022, 10, 28, 13, 14, 27, 4)\n",
      "(6, '2022-05-16 17:55:43', '97.253.103.59', '77.16.101.53', 26562, 17416, 'TCP', 379, 'Data', 'DNS', 'Qui numquam inventore repellat ratione fugit odit. Quidem est possimus voluptates reprehenderit vitae a. Quibusdam in itaque rerum.\\nExcepturi quisquam iusto provident adipisci.', 'No Detection', 31.55, 'no', 'DDoS', 'Known Pattern B', 'Ignored', 'High', 'Ehsaan Dalal', 'Segment A', 'Eluru, Manipur', 'No proxy', 'Log Data', 'No Data', 'Server', 'Opera', 'Linux', 2022, 5, 16, 17, 55, 43, 0)\n",
      "(7, '2023-02-12 07:13:17', '11.48.99.245', '178.157.14.116', 34489, 20396, 'ICMP', 1022, 'Data', 'DNS', 'Amet libero optio quidem praesentium libero. Ea magnam atque corporis ipsum iure iusto.\\nEveniet dolor odio libero. Minus iste fugit asperiores minima eos ipsum.', 'IoC Detected', 54.05, 'yes', 'Intrusion', 'Known Pattern A', 'Logged', 'High', 'Yuvaan Dubey', 'Segment A', 'Phagwara, Andhra Pradesh', '192.31.159.5', 'Log Data', 'Alert Data', 'Firewall', 'Mozilla', 'Macintosh', 2023, 2, 12, 7, 13, 17, 6)\n",
      "(8, '2023-06-27 11:02:56', '49.32.208.167', '72.202.237.9', 56296, 20857, 'TCP', 1281, 'Control', 'FTP', 'Veritatis nihil amet atque molestias aperiam minus. Velit hic aperiam iusto vitae nulla dolor maxime. Atque aspernatur reiciendis in.', 'IoC Detected', 56.34, 'yes', 'Intrusion', 'Known Pattern A', 'Blocked', 'High', 'Zaina Iyer', 'Segment B', 'Ambala, Tripura', 'No proxy', 'Log Data', 'Alert Data', 'Server', 'Mozilla', 'Macintosh', 2023, 6, 27, 11, 2, 56, 1)\n",
      "(9, '2021-08-15 22:29:04', '114.109.149.113', '160.88.194.172', 37918, 50039, 'UDP', 224, 'Data', 'HTTP', 'Consequatur ipsum autem reprehenderit quae. Doloribus dicta laboriosam porro consequatur dicta deleniti. Hic doloribus non aliquam.', 'No Detection', 16.51, 'yes', 'Malware', 'Known Pattern B', 'Blocked', 'Medium', 'Mishti Chaudhuri', 'Segment A', 'Rampur, Mizoram', '87.128.245.244', 'No Data', 'No Data', 'Server', 'Mozilla', 'Windows', 2021, 8, 15, 22, 29, 4, 6)\n"
     ]
    }
   ],
   "source": [
    "# Selecting the first 10 attacks from the table Attacks\n",
    "if __name__ == \"__main__\":\n",
    "    # Query\n",
    "    First10Attacks = 'SELECT * FROM \"Attacks\" LIMIT 10;'\n",
    "    execute_query(First10Attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Unnamed: 0',)\n",
      "('Source Port',)\n",
      "('Destination Port',)\n",
      "('Packet Length',)\n",
      "('Anomaly Scores',)\n",
      "('Year',)\n",
      "('Month',)\n",
      "('Day',)\n",
      "('Hour',)\n",
      "('Minute',)\n",
      "('Second',)\n",
      "('DayOfWeek',)\n",
      "('Device/OS',)\n",
      "('Alerts/Warnings',)\n",
      "('Attack Type',)\n",
      "('Attack Signature',)\n",
      "('Action Taken',)\n",
      "('Severity Level',)\n",
      "('User Information',)\n",
      "('Network Segment',)\n",
      "('Geo-location Data',)\n",
      "('Proxy Information',)\n",
      "('Firewall Logs',)\n",
      "('Timestamp',)\n",
      "('Source IP Address',)\n",
      "('Destination IP Address',)\n",
      "('IDS/IPS Alerts',)\n",
      "('Log Source',)\n",
      "('Protocol',)\n",
      "('Browser',)\n",
      "('Packet Type',)\n",
      "('Traffic Type',)\n",
      "('Payload Data',)\n",
      "('Malware Indicators',)\n"
     ]
    }
   ],
   "source": [
    "# Extracts the name of columns from table Attacks\n",
    "if __name__ == \"__main__\":\n",
    "    # Query\n",
    "    ColName = \"SELECT column_name FROM information_schema.columns WHERE table_name = 'Attacks';\" \n",
    "    execute_query(ColName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to executes a SQL query on a PostgreSQL database and to returns the results as a Pandas DataFrame\n",
    "The `execute_query_to_dataframe` function executes a SQL query on a PostgreSQL database and returns the results as a Pandas DataFrame. \n",
    "\n",
    "##### **Key Features**:\n",
    "\n",
    "1. **Database Connection**:\n",
    "   - Connects to the PostgreSQL database using the provided `db_config` dictionary, which contains credentials.\n",
    "\n",
    "2. **Query Execution**:\n",
    "   - Executes the given SQL query (`query`) and fetches the results using `pd.read_sql_query`, which converts the query results directly into a Pandas DataFrame.\n",
    "\n",
    "3. **Error Handling**:\n",
    "   - Catches and displays any errors that occur during execution, returning `None` in case of failure.\n",
    "\n",
    "4. **Resource Management**:\n",
    "   - Ensures the database connection is properly closed after the query execution using Python's `with` statement.\n",
    "\n",
    "##### **Use Case**:\n",
    "This function is ideal for scenarios where query results need to be further analyzed, visualized, or manipulated using Pandas, providing a seamless bridge between SQL and Python's data analysis ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query_to_dataframe(query, db_config):\n",
    "    try:\n",
    "        # Conneciton to the database\n",
    "        with psycopg2.connect(**db_config) as conn:\n",
    "            df = pd.read_sql_query(query, conn) # Execute the query and extract it in a pandas dataframe\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Error executing query:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting and saving data for the EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query `extractData4EDA` is designed to extract and enrich data from three tables: **Attacks** (the dataset to be enriched), **Targets**, and **Hackers** (the tables providing enrichment data). \n",
    "\n",
    "This query plays a critical role in the **data preparation step**, which involves processing and cleaning the raw data to ensure it is ready for **Exploratory Data Analysis (EDA)**. By combining these tables, the enriched dataset provides a more comprehensive view, enabling deeper and more meaningful insights during the analysis phase.\n",
    "\n",
    "##### **Purpose of the Query**:\n",
    "1. **Data Enrichment**:\n",
    "   - Combines relevant fields from the three tables using **LEFT JOINs**, ensuring all records from the `Attacks` table are retained, even if there are no matching records in the `Targets` or `Hackers` tables. This approach ensures we preserve the integrity of the raw data for complete analysis.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - Selects specific columns from the `Targets`, `Hackers`, and `Attacks` tables that are relevant for analysis. This includes information such as geolocation, threat scores, and attack attributes.\n",
    "\n",
    "3. **Preparation for Cleaning and Analysis**:\n",
    "   - The resulting dataset serves as a single, consolidated source of truth for subsequent data cleaning, transformation, and exploratory tasks.\n",
    "\n",
    "##### **Why Process and Clean Data Before EDA?**\n",
    "- **Ensure Data Quality**: Cleaning the data (e.g., handling missing values, removing duplicates, or correcting inconsistencies) improves its reliability.\n",
    "- **Streamline EDA**: By resolving data issues upfront, we can focus on uncovering meaningful patterns and insights without being hindered by errors or noise in the data.\n",
    "- **Optimize Analysis**: Consolidated and cleaned data enables more accurate statistical computations and visualizations, enhancing the overall EDA process.\n",
    "\n",
    "This query helps establish a robust foundation for extracting knowledge from the data, aligning with best practices in data science workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loren\\AppData\\Local\\Temp\\ipykernel_13324\\2042812010.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn) # Execute the query and extract it in a pandas dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            vic_ip vic_continent_code vic_continent_name vic_country_code2  \\\n",
      "0     84.9.164.252                 EU             Europe                GB   \n",
      "1   66.191.137.154               None      North America                US   \n",
      "2    198.219.82.17               None      North America                US   \n",
      "3  101.228.192.255                 AS               Asia                CN   \n",
      "4  189.243.174.238               None      North America                MX   \n",
      "\n",
      "  vic_country_code3 vic_country_name  \\\n",
      "0               GBR   United Kingdom   \n",
      "1               USA    United States   \n",
      "2               USA    United States   \n",
      "3               CHN            China   \n",
      "4               MEX           Mexico   \n",
      "\n",
      "                           vic_country_name_official vic_is_eu  \\\n",
      "0  United Kingdom of Great Britain and Northern I...     False   \n",
      "1                           United States of America     False   \n",
      "2                           United States of America     False   \n",
      "3                         People’s Republic of China     False   \n",
      "4                     United Mexican States (Mexico)     False   \n",
      "\n",
      "     vic_state_prov vic_state_code  ... Alerts/Warnings Attack Type  \\\n",
      "0           England         GB-ENG  ...              no     Malware   \n",
      "1         Minnesota          US-MN  ...              no     Malware   \n",
      "2           Alabama          US-AL  ...             yes        DDoS   \n",
      "3          Shanghai          CN-SH  ...             yes     Malware   \n",
      "4  Ciudad De México        MX-CDMX  ...             yes        DDoS   \n",
      "\n",
      "  Action Taken  Severity Level  Log Source  Browser  Device/OS  Year Month Day  \n",
      "0       Logged             Low      Server  Mozilla    Windows  2023     5  30  \n",
      "1      Blocked             Low    Firewall  Mozilla    Windows  2020     8  26  \n",
      "2      Ignored             Low    Firewall  Mozilla    Windows  2022    11  13  \n",
      "3      Blocked          Medium    Firewall  Mozilla  Macintosh  2023     7   2  \n",
      "4      Blocked             Low    Firewall  Mozilla    Windows  2023     7  16  \n",
      "\n",
      "[5 rows x 68 columns]\n"
     ]
    }
   ],
   "source": [
    "extractData4EDA = \"\"\"\n",
    "SELECT\n",
    "    -- Columns from 'Targets'\n",
    "    t.vic_ip, t.vic_continent_code, t.vic_continent_name, t.vic_country_code2,\n",
    "    t.vic_country_code3, t.vic_country_name, t.vic_country_name_official,\n",
    "    t.vic_is_eu, t.vic_state_prov, t.vic_state_code, t.vic_district, t.vic_city,\n",
    "    t.vic_zipcode, t.vic_latitude, t.vic_longitude, t.vic_threat_score, t.vic_is_tor,\n",
    "    t.vic_is_proxy, t.vic_proxy_type, t.vic_is_anonymous, t.vic_is_known_attacker,\n",
    "    t.vic_is_spam, t.vic_is_bot, t.vic_is_cloud_provider, t.vic_message,\n",
    "    -- Columns from 'Hackers'\n",
    "    h.att_ip, h.att_continent_code, h.att_continent_name, h.att_country_code2,\n",
    "    h.att_country_code3, h.att_country_name, h.att_country_name_official,\n",
    "    h.att_is_eu, h.att_state_prov, h.att_state_code, h.att_district, h.att_city,\n",
    "    h.att_zipcode, h.att_latitude, h.att_longitude, h.att_threat_score, h.att_is_tor,\n",
    "    h.att_is_proxy, h.att_proxy_type, h.att_is_anonymous, h.att_is_known_attacker,\n",
    "    h.att_is_spam, h.att_is_bot, h.att_is_cloud_provider, h.att_message,\n",
    "    -- Columns from 'Attacks'\n",
    "    a.\"Source IP Address\", a.\"Destination IP Address\", a.\"Protocol\", a.\"Packet Type\",\n",
    "    a.\"Packet Length\", a.\"Traffic Type\", a.\"Malware Indicators\", a.\"Anomaly Scores\", a.\"Alerts/Warnings\",\n",
    "    a.\"Attack Type\", a.\"Action Taken\", a.\"Severity Level\", a.\"Log Source\", a.\"Browser\", a.\"Device/OS\",\n",
    "    a.\"Year\", a.\"Month\", a.\"Day\"\n",
    "FROM \n",
    "    \"Attacks\" a\n",
    "LEFT JOIN \n",
    "    \"Targets\" t ON a.\"Destination IP Address\" = t.vic_ip\n",
    "LEFT JOIN \n",
    "    \"Hackers\" h ON a.\"Source IP Address\" = h.att_ip;\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data4EDA = execute_query_to_dataframe(extractData4EDA, db_config)\n",
    "    if data4EDA is not None:\n",
    "        print(data4EDA.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Useful checks on the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 68)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the dataset \n",
    "data4EDA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['vic_ip', 'vic_continent_code', 'vic_continent_name',\n",
       "       'vic_country_code2', 'vic_country_code3', 'vic_country_name',\n",
       "       'vic_country_name_official', 'vic_is_eu', 'vic_state_prov',\n",
       "       'vic_state_code', 'vic_district', 'vic_city', 'vic_zipcode',\n",
       "       'vic_latitude', 'vic_longitude', 'vic_threat_score', 'vic_is_tor',\n",
       "       'vic_is_proxy', 'vic_proxy_type', 'vic_is_anonymous',\n",
       "       'vic_is_known_attacker', 'vic_is_spam', 'vic_is_bot',\n",
       "       'vic_is_cloud_provider', 'vic_message', 'att_ip', 'att_continent_code',\n",
       "       'att_continent_name', 'att_country_code2', 'att_country_code3',\n",
       "       'att_country_name', 'att_country_name_official', 'att_is_eu',\n",
       "       'att_state_prov', 'att_state_code', 'att_district', 'att_city',\n",
       "       'att_zipcode', 'att_latitude', 'att_longitude', 'att_threat_score',\n",
       "       'att_is_tor', 'att_is_proxy', 'att_proxy_type', 'att_is_anonymous',\n",
       "       'att_is_known_attacker', 'att_is_spam', 'att_is_bot',\n",
       "       'att_is_cloud_provider', 'att_message', 'Source IP Address',\n",
       "       'Destination IP Address', 'Protocol', 'Packet Type', 'Packet Length',\n",
       "       'Traffic Type', 'Malware Indicators', 'Anomaly Scores',\n",
       "       'Alerts/Warnings', 'Attack Type', 'Action Taken', 'Severity Level',\n",
       "       'Log Source', 'Browser', 'Device/OS', 'Year', 'Month', 'Day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes the columns\n",
    "data4EDA.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving of the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File successfully saved!!!\n",
      "Your file location:D:\\UniVault\\UniProjects\\DataScience\\Year_1\\DataMan\\data\\csv_file\\final_df_ExtractedWithPostgreSQL.csv\n"
     ]
    }
   ],
   "source": [
    "full_destination_path = r\"\"\n",
    "data4EDA.to_csv(full_destination_path, index = False)\n",
    "print(\"File successfully saved!!!\")\n",
    "print(f\"{\"Your file location:\"}{full_destination_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ⏭️ Next Step: \n",
    "$\\rightarrow$ Data Preparation (see 3_Data_Prepration.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
